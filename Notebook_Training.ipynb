{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVISO:\n",
    "Este notebook es provisional, la versión final estará en training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para 'intents.json'\n",
    "import json # Para el formato json\n",
    "import codecs # Lectura de caracteres en español\n",
    "\n",
    "# Procesamiento de Lenguaje Natural\n",
    "import spacy # Lemmatizer (convertir palabras) con lenguaje español\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk # Natural Language ToolKit: Tokenizar\n",
    "\n",
    "# Redes Neuronales:\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿ persona ? , ¿ estar ahí ?\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def lemmatizer(text):  \n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma_ for word in doc])\n",
    "\n",
    "text = '¿personas?, ¿estás ahí?'\n",
    "print(lemmatizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(codecs.open('intents.json', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Buenas', 'Buenos', 'Hey', 'Hi', 'Hola', 'aqui', 'aquí', 'donde', 'día', 'dónde', 'estar', 'este', 'llegar', 'llevar', 'llévame', 'lugar', 'noche', 'poder', 'quedar', 'tarde', '¿ cómo', '¿ dónde', '¿ saber', '¿ yo'] \n",
      "\n",
      "Tags: ['Consulta_Lugar', 'Saludos'] \n",
      "\n",
      "Documents: [(['Hola'], 'Saludos'), (['Hey'], 'Saludos'), (['Hi'], 'Saludos'), (['Buenos', 'días'], 'Saludos'), (['Buenas', 'tardes'], 'Saludos'), (['Buenas', 'noches'], 'Saludos'), (['Buenas'], 'Saludos'), (['¿Sabes', 'donde', 'está', '?'], 'Consulta_Lugar'), (['¿Dónde', 'queda', '?'], 'Consulta_Lugar'), (['¿Dónde', 'queda', 'este', 'lugar', '?'], 'Consulta_Lugar'), (['¿Cómo', 'puedo', 'llegar', '?'], 'Consulta_Lugar'), (['¿sabes', 'dónde', 'queda', '?'], 'Consulta_Lugar'), (['¿me', 'puedes', 'llevar', 'aqui', '?'], 'Consulta_Lugar'), (['llévame', 'aquí'], 'Consulta_Lugar')]\n"
     ]
    }
   ],
   "source": [
    "words = [] # Palabras individuales usadas\n",
    "tags = [] # Etiquetas de intención\n",
    "ignore_letters = ['¿', '?', '.', '!', '(', ')']\n",
    "documents = [] # Lista de tuplas ([Lista de palabras], etiqueta asociada de intención)\n",
    "for intent in intents['intents']:\n",
    "    # Por cada patrón:\n",
    "    for pattern in intent['patterns']:\n",
    "        #pattern = lemmatizer(pattern)\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in tags:\n",
    "            tags.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))\n",
    "tags = sorted(set(tags))\n",
    "print(\"Words: %s \\n\"%words)\n",
    "print(\"Tags: %s \\n\"%tags)\n",
    "print(\"Documents:\", documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1]], [[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1]], [[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1]], [[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 1]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1]], [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0], [1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0], [1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0], [1, 0]], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0], [1, 0]], [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], [1, 0]], [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0]]]\n"
     ]
    }
   ],
   "source": [
    "training = []\n",
    "output_empty = [0] * len(tags)\n",
    "\n",
    "for document in documents:\n",
    "    bag = [] # bag of words\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word.lower() in word_patterns else bag.append(0)\n",
    "    \n",
    "    output_row = list(output_empty)\n",
    "    output_row[tags.index(document[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "print(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
