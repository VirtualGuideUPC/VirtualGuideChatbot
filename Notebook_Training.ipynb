{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AVISO:\n",
    "Este notebook es provisional, la versión final es training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy # Para Lemmatizer con lenguaje español\n",
    "import json # Para 'intents.json'\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk # Natural Language ToolKit, que permite tokenizar\n",
    "\n",
    "# Redes Neuronales:\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persona\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "def lemmatizer(text):  \n",
    "    doc = nlp(text)\n",
    "    return ' '.join([word.lemma_ for word in doc])\n",
    "\n",
    "text = 'personas'\n",
    "print(lemmatizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = json.loads(open('intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'Hey', 'Hi', 'Buenos', 'dias', 'Buenas', 'tarde', 'Buenas', 'noche', 'Buenas', 'saber', 'donde', 'estã ¡', 'â¿dã³ndir', 'quedar', 'donde', 'quedar', 'este', 'lugar', 'como', 'poder', 'llegar', 'saber', 'donde', 'quedar', 'yo', 'poder', 'llevar', 'aqui', 'llevame', 'aqui']\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Buenas',\n",
       " 'Buenos',\n",
       " 'Hey',\n",
       " 'Hi',\n",
       " 'Hola',\n",
       " 'aqui',\n",
       " 'como',\n",
       " 'dias',\n",
       " 'donde',\n",
       " 'este',\n",
       " 'estã ¡',\n",
       " 'llegar',\n",
       " 'llevame',\n",
       " 'llevar',\n",
       " 'lugar',\n",
       " 'noche',\n",
       " 'poder',\n",
       " 'quedar',\n",
       " 'saber',\n",
       " 'tarde',\n",
       " 'yo',\n",
       " 'â¿dã³ndir']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [] # Palabras individuales usadas\n",
    "tags = [] # Etiquetas de intención\n",
    "ignore_letters = ['¿', '?', '.', '!', '(', ')']\n",
    "documents = [] # Lista de tuplas ([Lista de palabras], etiqueta asociada de intención)\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #print(pattern)\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        #print(word_list)\n",
    "        #words.append(word_list)\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in tags:\n",
    "            tags.append(intent['tag'])\n",
    "            \n",
    "words = [lemmatizer(word) for word in words if word not in ignore_letters]\n",
    "print(words)\n",
    "print(\"---\")\n",
    "words = sorted(set(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
